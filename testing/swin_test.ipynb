{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIN TESTING\n",
    "\n",
    "This notebook is used to test the SWIN model on the huggingface/transformers repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and reproducing the feature extractor\n",
    "\n",
    "In this section we test and figure out the structure of the feature extractor used in the huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image, ImageMath\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows us the structure of the feature extractor of swin transformer. It includes first a resizing using billinear interpolation, then a normalization using the mean and std of the imagenet dataset, and finally a conversion to tensor. The feature extractor type is merely a notion of what type of feature is returned (image or text whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "# vit_feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "# print(feature_extractor)\n",
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download a test image to see the effects of the feature extractor and our replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is us trying to replicate the feature extractor. We use the same resizing and normalization, but we use torchvision to convert the image to a tensor and to take care of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3138,  0.4337,  0.4679,  ..., -0.3541, -0.3369, -0.3369],\n",
       "         [ 0.3652,  0.4337,  0.4679,  ..., -0.3541, -0.3541, -0.3883],\n",
       "         [ 0.3138,  0.3994,  0.4166,  ..., -0.4397, -0.4226, -0.4054],\n",
       "         ...,\n",
       "         [ 1.8893,  1.7865,  1.6667,  ...,  1.5982,  1.4783,  1.4098],\n",
       "         [ 1.8722,  1.8037,  1.7523,  ...,  1.3413,  1.0844,  0.9303],\n",
       "         [ 1.8550,  1.7180,  1.7180,  ...,  0.2282, -0.0458, -0.3541]],\n",
       "\n",
       "        [[-1.5980, -1.6155, -1.6155,  ..., -1.7906, -1.7906, -1.8081],\n",
       "         [-1.5630, -1.5630, -1.5630,  ..., -1.7556, -1.7556, -1.7731],\n",
       "         [-1.6155, -1.5980, -1.5630,  ..., -1.7906, -1.7906, -1.7906],\n",
       "         ...,\n",
       "         [-0.4076, -0.5126, -0.6176,  ..., -0.7577, -0.8277, -0.8803],\n",
       "         [-0.4076, -0.4601, -0.5651,  ..., -0.8803, -1.0203, -1.0903],\n",
       "         [-0.4251, -0.5651, -0.5826,  ..., -1.4405, -1.5455, -1.6681]],\n",
       "\n",
       "        [[-0.7936, -0.6193, -0.6541,  ..., -1.2293, -1.1247, -1.1770],\n",
       "         [-0.8110, -0.7238, -0.6715,  ..., -1.2293, -1.1596, -1.2293],\n",
       "         [-0.7413, -0.6541, -0.6193,  ..., -1.2467, -1.2467, -1.2816],\n",
       "         ...,\n",
       "         [ 1.6814,  1.6465,  1.4200,  ...,  1.4025,  1.2805,  1.1411],\n",
       "         [ 1.6291,  1.5071,  1.5594,  ...,  1.0888,  0.8797,  0.7054],\n",
       "         [ 1.6814,  1.6640,  1.5594,  ..., -0.0267, -0.5321, -0.7587]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMALIZE_MEAN = feature_extractor.image_mean\n",
    "NORMALIZE_STD = feature_extractor.image_std\n",
    "\n",
    "PILToTensor = transforms.PILToTensor()\n",
    "normalize = transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
    "\n",
    "transform_test_image1 = image\n",
    "transform_test_image1 = transform_test_image1.resize((224, 224), 2)\n",
    "# transform_test_image = transforms.resize(transform_test_image, (224, 224), 2)\n",
    "transform_test_image1 = PILToTensor(transform_test_image1)\n",
    "transform_test_image1 = transform_test_image1.float() / 255.0\n",
    "transform_test_image1 = normalize(transform_test_image1)\n",
    "transform_test_image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3309,  0.4337,  0.4679,  ..., -0.3541, -0.3369, -0.3369],\n",
       "         [ 0.3652,  0.4337,  0.4679,  ..., -0.3541, -0.3541, -0.3883],\n",
       "         [ 0.3138,  0.3994,  0.4166,  ..., -0.4397, -0.4226, -0.4054],\n",
       "         ...,\n",
       "         [ 1.8893,  1.7865,  1.6667,  ...,  1.5982,  1.4783,  1.4098],\n",
       "         [ 1.8722,  1.8037,  1.7352,  ...,  1.3242,  1.0844,  0.9303],\n",
       "         [ 1.8722,  1.7180,  1.7180,  ...,  0.2282, -0.0629, -0.3541]],\n",
       "\n",
       "        [[-1.5980, -1.6155, -1.6155,  ..., -1.7906, -1.7906, -1.8081],\n",
       "         [-1.5455, -1.5805, -1.5630,  ..., -1.7731, -1.7556, -1.7731],\n",
       "         [-1.6155, -1.5980, -1.5630,  ..., -1.7906, -1.7906, -1.7906],\n",
       "         ...,\n",
       "         [-0.4076, -0.5301, -0.6176,  ..., -0.7402, -0.8102, -0.8803],\n",
       "         [-0.4076, -0.4601, -0.5651,  ..., -0.8803, -1.0203, -1.0903],\n",
       "         [-0.4251, -0.5651, -0.5826,  ..., -1.4405, -1.5455, -1.6681]],\n",
       "\n",
       "        [[-0.7936, -0.6193, -0.6541,  ..., -1.2293, -1.1247, -1.1596],\n",
       "         [-0.8110, -0.7238, -0.6715,  ..., -1.2293, -1.1596, -1.2293],\n",
       "         [-0.7413, -0.6541, -0.6193,  ..., -1.2467, -1.2467, -1.2816],\n",
       "         ...,\n",
       "         [ 1.6814,  1.6465,  1.4200,  ...,  1.4025,  1.2805,  1.1411],\n",
       "         [ 1.6291,  1.5071,  1.5594,  ...,  1.0888,  0.8797,  0.7228],\n",
       "         [ 1.6988,  1.6640,  1.5594,  ..., -0.0441, -0.5321, -0.7587]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PILToTensor = transforms.PILToTensor()\n",
    "normalize = transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
    "resize = transforms.Resize((224, 224), transforms.InterpolationMode.BILINEAR, antialias=True)\n",
    "to_float = transforms.ConvertImageDtype(torch.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Resize((224, 224), transforms.InterpolationMode.BILINEAR, antialias=False),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)\n",
    "])\n",
    "\n",
    "transform_test_image2 = image\n",
    "transform_test_image2 = PILToTensor(transform_test_image2)\n",
    "transform_test_image2 = resize(transform_test_image2)\n",
    "transform_test_image2 = to_float(transform_test_image2)\n",
    "transform_test_image2 = normalize(transform_test_image2)\n",
    "transform_test_image2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try using the feature extractor on the image. We see that the output is the same as the custom preprocessing we constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3138,  0.4337,  0.4851,  ..., -0.3541, -0.3369, -0.3541],\n",
       "          [ 0.3652,  0.4337,  0.4679,  ..., -0.3541, -0.3541, -0.3883],\n",
       "          [ 0.3138,  0.3994,  0.4166,  ..., -0.4568, -0.4226, -0.3883],\n",
       "          ...,\n",
       "          [ 1.9064,  1.7865,  1.6495,  ...,  1.6153,  1.4954,  1.4440],\n",
       "          [ 1.8722,  1.8037,  1.7523,  ...,  1.4098,  1.1358,  0.9817],\n",
       "          [ 1.8722,  1.7180,  1.7352,  ...,  0.1254, -0.1657, -0.4739]],\n",
       "\n",
       "         [[-1.6155, -1.6155, -1.6155,  ..., -1.7906, -1.7906, -1.8081],\n",
       "          [-1.5630, -1.5630, -1.5630,  ..., -1.7731, -1.7556, -1.7731],\n",
       "          [-1.6331, -1.5980, -1.5630,  ..., -1.8081, -1.7906, -1.7906],\n",
       "          ...,\n",
       "          [-0.3901, -0.5301, -0.6352,  ..., -0.7402, -0.8102, -0.8627],\n",
       "          [-0.3901, -0.4426, -0.5651,  ..., -0.8452, -1.0028, -1.0728],\n",
       "          [-0.4251, -0.5651, -0.5826,  ..., -1.4930, -1.5980, -1.7206]],\n",
       "\n",
       "         [[-0.7936, -0.6018, -0.6541,  ..., -1.2293, -1.1247, -1.1596],\n",
       "          [-0.8458, -0.7238, -0.6890,  ..., -1.2293, -1.1596, -1.2293],\n",
       "          [-0.7413, -0.6367, -0.6018,  ..., -1.2467, -1.2641, -1.2816],\n",
       "          ...,\n",
       "          [ 1.6814,  1.6640,  1.3851,  ...,  1.4374,  1.3154,  1.1759],\n",
       "          [ 1.6465,  1.4548,  1.5594,  ...,  1.1585,  0.9319,  0.7751],\n",
       "          [ 1.6988,  1.6988,  1.5594,  ..., -0.1487, -0.6715, -0.8981]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "inputs[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor(68662)\n",
      "tensor(0.0175)\n",
      "tensor(0.0023)\n"
     ]
    }
   ],
   "source": [
    "embedding_difference = (transform_test_image1 - transform_test_image2).abs()\n",
    "print(embedding_difference.mean().le(1e-3).item())\n",
    "print(embedding_difference.argmax())\n",
    "print(embedding_difference.max())\n",
    "print(embedding_difference.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All thats left is to package this into a pytorch transformation module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3309,  0.4337,  0.4679,  ..., -0.3712, -0.3198, -0.2171],\n",
       "         [ 0.4679,  0.4166,  0.4508,  ..., -0.4054, -0.3369, -0.4397],\n",
       "         [ 0.2624,  0.4166,  0.3823,  ..., -0.4911, -0.5424, -0.5082],\n",
       "         ...,\n",
       "         [ 1.8722,  1.7523,  1.5982,  ...,  1.6153,  1.4612,  1.3927],\n",
       "         [ 1.8379,  1.8208,  1.7523,  ...,  1.3927,  1.1872,  0.9817],\n",
       "         [ 1.8893,  1.6495,  1.6667,  ..., -0.0629, -0.2171, -0.5767]],\n",
       "\n",
       "        [[-1.5980, -1.6331, -1.5980,  ..., -1.8256, -1.8081, -1.7731],\n",
       "         [-1.4405, -1.5805, -1.5455,  ..., -1.7556, -1.7206, -1.6856],\n",
       "         [-1.6856, -1.5980, -1.5980,  ..., -1.7906, -1.8081, -1.8957],\n",
       "         ...,\n",
       "         [-0.4426, -0.6176, -0.6176,  ..., -0.7752, -0.8627, -0.9678],\n",
       "         [-0.4251, -0.4251, -0.5826,  ..., -0.7927, -0.9153, -1.0203],\n",
       "         [-0.4251, -0.6176, -0.6352,  ..., -1.5805, -1.6506, -1.8081]],\n",
       "\n",
       "        [[-0.8110, -0.5321, -0.5844,  ..., -1.2816, -1.1073, -1.1073],\n",
       "         [-0.6890, -0.8284, -0.7413,  ..., -1.2467, -1.1247, -1.1944],\n",
       "         [-0.8110, -0.5844, -0.6018,  ..., -1.2293, -1.2990, -1.3861],\n",
       "         ...,\n",
       "         [ 1.7860,  1.8383,  1.3328,  ...,  1.4025,  1.2457,  1.0017],\n",
       "         [ 1.5594,  1.4897,  1.6291,  ...,  1.1237,  1.0539,  0.9145],\n",
       "         [ 1.7337,  1.6988,  1.4897,  ..., -0.2881, -0.7761, -1.0201]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_preprocessor = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Resize((224, 224), 2),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "swin_preprocessor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is not possible to completelt recreate the embedding that SWIN uses using only pytorch modules, and therefore we will use the transformers library to do handle encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a2807571ceb5298b2f1983d1350812d9420f551039ce5a6ed3c5736a180364b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
