4,5,6,7
Running convnext training on 4,5,6,7
11.6
True
4
creating checkpoint.
initializing ConvNext model.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
11.6
True
4
creating checkpoint.
initializing ConvNext model.
11.6
True
4
creating checkpoint.
initializing ConvNext model.
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
11.6
True
4
creating checkpoint.
initializing ConvNext model.
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

preparing CIFAR10 dataset.
Files already downloaded and verified
Files already downloaded and verified
creating data loaders.
fitting model.
preparing CIFAR10 dataset.
Files already downloaded and verified
Files already downloaded and verified
creating data loaders.
fitting model.
preparing CIFAR10 dataset.
Files already downloaded and verified
Files already downloaded and verified
creating data loaders.
fitting model.
preparing CIFAR10 dataset.
Files already downloaded and verified
Files already downloaded and verified
creating data loaders.
fitting model.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [4,5,6,7]

  | Name    | Type             | Params
---------------------------------------------
0 | loss_fn | CrossEntropyLoss | 0     
1 | model   | ConvNeXt         | 87.6 M
---------------------------------------------
87.6 M    Trainable params
0         Non-trainable params
87.6 M    Total params
350.307   Total estimated model params size (MB)
/home/dwp992/miniconda3/envs/ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/home/dwp992/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1024, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [1024, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/dwp992/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1024, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [1024, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/dwp992/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1024, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [1024, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/dwp992/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1024, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [1024, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484803030/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
`Trainer.fit` stopped: `max_epochs=30` reached.
