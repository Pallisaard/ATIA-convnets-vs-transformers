{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Preprocessing\n",
    "\n",
    "In this notebook we cover the preprocessing for the Cifar10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10 import get_cifar10_data\n",
    "import torchvision.transforms as T\n",
    "from timm import create_model\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from transformers import AutoModelForImageClassification\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening the data and running a transformed image through each model. We utilise the create_transform function to create a transform for each model. We try using the BiT suggested transformation of transforming images of size below 96x96 to 128x128. We suspect well enough that this transformation will not work using the SWIN transformer, but alas we need to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = get_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "example_image, example_label = train_data[0]\n",
    "print(example_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cifar10_transform = create_transform(224, is_training=False)\n",
    "convnext_model = create_model(\"convnext_base_in22k\", pretrained=True, num_classes=10)\n",
    "swin_model = AutoModelForImageClassification.from_pretrained(\n",
    "        \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
    "        num_labels=10,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = cifar10_transform(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1638,  0.0234,  0.1389,  0.5137,  0.5145, -0.3699,  0.3191,  0.1650,\n",
       "          0.0802,  0.0093]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnext_output = convnext_model(model_input.unsqueeze(0))\n",
    "convnext_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3066,  0.3679,  0.3508,  0.3193,  0.3030,  0.2363,  0.0645,  0.5388,\n",
       "         -0.5157, -0.3150]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_output = swin_model(model_input.unsqueeze(0))\n",
    "swin_output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this does very well not work. We therefore decide to use 128x128 for the ConvNext and 224X224 for SWIN. On the topic of transformations for cifar10 we choose not to use any random cropping as we scale the image up anyway, making random cropping seriously moot since the we essentially could just choose to not upscale it to such high resolution and therefore keep all the information. With this being decided it actually ends up being the case that our predefined transformation for the SWIN model works best for both models. It is defined below and is used in the cifar10 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_feature_extractor(image_size=(224, 224)):\n",
    "    return T.Compose([\n",
    "        T.PILToTensor(),\n",
    "        T.Resize(image_size, T.InterpolationMode.BILINEAR, antialias=False),\n",
    "        T.ConvertImageDtype(torch.float32),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a2807571ceb5298b2f1983d1350812d9420f551039ce5a6ed3c5736a180364b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
